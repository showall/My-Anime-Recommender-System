{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/3325/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/rating.csv')\n",
    "df_content = pd.read_csv('data/anime.csv')\n",
    "#del df['Unnamed: 0']\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"user_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_content[[\"anime_id\",\"name\"]], on=\"anime_id\").dropna().drop([\"name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show df_content to get an idea of the data\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content[\"rating\"] = df_content[\"rating\"].fillna(0)\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"anime_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"user_id\"]).count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"user_id\",\"anime_id\"]).count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.anime_id) #number of times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and explore duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "df_content=df_content[~df_content.duplicated(subset=[\"anime_id\"])==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df.user_id !=\"\"][\"anime_id\"].unique()))\n",
    "print(len(df_content[\"anime_id\"]))\n",
    "print(len(df[df.user_id !=\"\"][\"user_id\"].dropna().unique()))\n",
    "print(len(df[[\"anime_id\",\"user_id\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content[\"rating\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_top_articles(n, df=df):\n",
    "#     '''\n",
    "#     INPUT:\n",
    "#     n - (int) the number of top articles to return\n",
    "#     df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "#     OUTPUT:\n",
    "#     top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "#     '''\n",
    "#     top_articles= list(set(df[(df.article_id).apply(lambda x:int(x)).isin(\n",
    "#         list(df[\"article_id\"].value_counts().head(n).index))][\"title\"]))\n",
    "    \n",
    "    \n",
    "#     return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "\n",
    "# def get_top_article_ids(n, df=df):\n",
    "#     '''\n",
    "#     INPUT:\n",
    "#     n - (int) the number of top articles to return\n",
    "#     df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "#     OUTPUT:\n",
    "#     top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "#     '''\n",
    "#     top_articles= list(set(df[(df.article_id).apply(lambda x:int(x)).isin(list(df[\"article_id\"].value_counts().head(n).index))][\"article_id\"].astype('str')))\n",
    " \n",
    "#     return top_articles # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_anime(n, df_content=df_content):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    top_anime= list(df_content.sort_values([\"rating\"],ascending=False,axis=0)[\"name\"].head(n))\n",
    "    top_anime_id = list(df_content.sort_values([\"rating\"],ascending=False,axis=0)[\"anime_id\"].head(n))\n",
    "    return top_anime, top_anime_id # Return the top article titles from df (not df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_anime , top_anime_id= get_top_anime(10)\n",
    "print(top_anime)\n",
    "print(top_anime_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "def create_user_item_matrix(df):\n",
    "    \n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "   #df = df[[]]\n",
    "    user_item = df.groupby(['user_id', 'anime_id']).count().unstack()\n",
    "    user_item = user_item.fillna(0)\n",
    "#    user_item[user_item > 1] = 1\n",
    "    return user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"user_item_.p\",\"wb\") as p:\n",
    "    pickle.dump(user_item, p)\n",
    "user_item  = pd.read_pickle('user_item_.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    similiarity = pd.DataFrame(np.dot(user_item[user_item.index == user_id],user_item.T).T, columns=[\"similiarity_score\"])\n",
    "    user_id_list = dict(zip(pd.DataFrame(user_item.index).index,pd.DataFrame(user_item.index)[\"user_id\"]))\n",
    "    similiarity.index = similiarity.index.map(user_id_list)    \n",
    "    similiarity = similiarity.reset_index().sort_values([\"similiarity_score\",\"index\"],ascending=[False,True])\n",
    "    # sort by similarity\n",
    "    # create list of just the ids\n",
    "    most_similar_users = list(similiarity[\"index\"])\n",
    "    # remove the own user's id\n",
    "    most_similar_users_exclude_own = [x for x in most_similar_users if x != user_id]  \n",
    "    return most_similar_users_exclude_own # return a list of the users in order from most to least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(33)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_names(anime_ids, df_content=df_content):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    anime_names = []\n",
    "    for id in anime_ids :\n",
    "        \n",
    "     #   article_names.append( df[df.article_id==id][\"title\"].unique()[0])    \n",
    "      #  print(id)\n",
    "        anime_names.append(df_content[df_content.anime_id.astype(\"float\")==id][\"name\"].values[0])\n",
    "    \n",
    "    return anime_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_animes(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    mask = user_item[user_item.index == user_id]\n",
    "    mask = list(mask.iloc[0])\n",
    "    anime_id = list(user_item.rating.reset_index().columns[1:])\n",
    "    id_freq_df = pd.DataFrame([anime_id, mask]).T\n",
    "    anime_ids = list(id_freq_df[id_freq_df[1]==1][0])\n",
    "   # article_ids = [str(x) for x in article_ids]\n",
    "    anime_names = get_anime_names(anime_ids)\n",
    "    \n",
    "    return anime_ids, anime_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    user_count = 0\n",
    "    recs = []\n",
    "    own_anime_ids, own_anime_names = get_user_animes(user_id, user_item=user_item)\n",
    "    similar_users = find_similar_users(user_id)\n",
    "    for j in range (1,1000):\n",
    "        similar_users_anime_ids, anime_names = get_user_animes(similar_users[j])\n",
    "        for i in [x for x in similar_users_anime_ids if (x not in own_anime_ids and x not in recs)]:\n",
    "            recs.append(i)\n",
    "        if len(recs)>m:\n",
    "            break\n",
    "    return recs[0:m] # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user_recs(user_id=4, m=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Results\n",
    "get_anime_names(user_user_recs(4, 20))# Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL\n",
    "\n",
    "# def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "#     '''\n",
    "#     INPUT:\n",
    "#     user_id - (int)\n",
    "#     df - (pandas dataframe) df as defined at the top of the notebook \n",
    "#     user_item - (pandas dataframe) matrix of users by articles: \n",
    "#             1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "#     OUTPUT:\n",
    "#     neighbors_df - (pandas dataframe) a dataframe with:\n",
    "#                     neighbor_id - is a neighbor user_id\n",
    "#                     similarity - measure of the similarity of each user to the provided user_id\n",
    "#                     num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "#     Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "#                     highest of each is higher in the dataframe\n",
    "     \n",
    "#     '''\n",
    "#     # Your code here\n",
    "\n",
    "#     neighbors_df = pd.DataFrame(np.dot(user_item[user_item.index == user_id],user_item.T).T, columns=[\"similiarity_score\"])\n",
    "#     user_id_list = dict(zip(pd.DataFrame(user_item.index).index,pd.DataFrame(user_item.index)[\"user_id\"]))\n",
    "#     neighbors_df.index = neighbors_df.index.map(user_id_list)    \n",
    "#     neighbors_df =neighbors_df.reset_index().sort_values([\"similiarity_score\",\"index\"],ascending=[False,True])\n",
    "#     neighbors_df.rename(columns = {'index':'neighbor_id'}, inplace = True)\n",
    "#     neighbors_df= neighbors_df.merge(df,left_on=\"neighbor_id\",right_on=\"user_id\").drop([\"user_id\",\"anime_id\"], axis=1)\n",
    "#     neighbors_df.rename(columns = {'rating':'num_interactions'}, inplace = True)\n",
    "#     neighbors_df =neighbors_df.sort_values([\"similiarity_score\",\"num_interactions\"], ascending=False)  \n",
    "#     neighbors_df = neighbors_df[neighbors_df[\"neighbor_id\"] != user_id]\n",
    "#     return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "# def user_user_recs_part2(user_id, m=10):\n",
    "#     '''\n",
    "#     INPUT:\n",
    "#     user_id - (int) a user id\n",
    "#     m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "#     OUTPUT:\n",
    "#     recs - (list) a list of recommendations for the user by article id\n",
    "#     rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "#     Description:\n",
    "#     Loops through the users based on closeness to the input user_id\n",
    "#     For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "#     Does this until m recommendations are found\n",
    "    \n",
    "#     Notes:\n",
    "#     * Choose the users that have the most total article interactions \n",
    "#     before choosing those with fewer article interactions.\n",
    "\n",
    "#     * Choose articles with the articles with the most total interactions \n",
    "#     before choosing those with fewer total interactions. \n",
    "   \n",
    "#     '''\n",
    "#     # Your code here\n",
    "  \n",
    "#     user_count = 0\n",
    "#     recs = []\n",
    "#     own_article_ids, own_article_names = get_user_articles(user_id, user_item=user_item)\n",
    "#     top_users = list(get_top_sorted_users(user_id)[\"neighbor_id\"])\n",
    "    \n",
    "#     for j in range (1,1000):\n",
    "#         if len(recs)>=m:\n",
    "#             break\n",
    "#         similar_users_article_ids, article_names = get_user_articles(top_users[j])\n",
    "#         similar_users_article_ids_int = [float(x) for x in similar_users_article_ids]\n",
    "#         df_sub = df[df[\"anime_id\"].apply(lambda x: x in similar_users_article_ids_int)]\n",
    "#         similar_users_article_ids = get_top_article_ids(len(similar_users_article_ids_int), df=df_sub)\n",
    "#         for i in [x for x in similar_users_article_ids if (x not in own_article_ids and x not in recs)]:\n",
    "#             if len(recs)>=m:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 recs.append(i)\n",
    "#     rec_names = get_article_names(recs, df=df)\n",
    "    \n",
    "    \n",
    "#     return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "\n",
    "    neighbors_df = pd.DataFrame(np.dot(user_item[user_item.index == 1],user_item.T).T, columns=[\"similiarity_score\"])\n",
    "    user_id_list = dict(zip(pd.DataFrame(user_item.index).index,pd.DataFrame(user_item.index)[\"user_id\"]))\n",
    "    neighbors_df.index = neighbors_df.index.map(user_id_list)    \n",
    "    neighbors_df =neighbors_df.reset_index().sort_values([\"similiarity_score\",\"index\"],ascending=[False,True])\n",
    "    neighbors_df.rename(columns = {'index':'neighbor_id'}, inplace = True)\n",
    "    #    neighbors_df= neighbors_df.merge(df,left_on=\"neighbor_id\",right_on=\"user_id\").drop([\"user_id\",\"anime_id\"], axis=1)\n",
    "    neighbors_df= neighbors_df.merge(df,left_on=\"neighbor_id\",right_on=\"user_id\")\n",
    "    neighbors_df =neighbors_df.sort_values([\"similiarity_score\",\"rating\"], ascending=False)  \n",
    "    neighbors_df = neighbors_df[neighbors_df[\"neighbor_id\"] != 1]\n",
    "    neighbors_df =neighbors_df.sort_values([\"similiarity_score\",\"rating\"], ascending=False)  \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "  \n",
    "    user_count = 0\n",
    "    recs = []\n",
    "    own_anime_ids, own_anime_names = get_user_animes(user_id, user_item=user_item)\n",
    "    top_anime_list = list(get_top_sorted_users(user_id)[\"anime_id\"])\n",
    "    \n",
    "    for j in range (1,1000):\n",
    "        if len(recs)>=m:\n",
    "            break\n",
    "        for i in [x for x in top_anime_list if (x not in own_anime_ids and x not in recs)]:\n",
    "            if len(recs)>=m:\n",
    "                break\n",
    "            else:\n",
    "                recs.append(i)\n",
    "    rec_names = get_anime_names(recs)\n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs, _ = get_top_anime(10)# Your recommendations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_anime(10)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, you will build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_by_rating= df.groupby(['user_id', 'anime_id'])['rating'].max().unstack()\n",
    "import pickle\n",
    "with open(\"user_item_matrix.p\",\"wb\") as p:\n",
    "    pickle.dump(user_item_matrix_by_rating, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "#import svd_tests as t\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_anime = list(df.anime_id.value_counts().index[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_subset = user_item_matrix_by_rating[listed_anime].dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mat = np.matrix(user_movie_subset[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # keep track of iteration and MSE\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUST SAMPLE DONT USE HERE, USE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mat, movie_mat = FunkSVD(rating_mat, \n",
    "                              latent_features=5, \n",
    "                              learning_rate=0.008, \n",
    "                              iters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(rating_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is: {}\".format(preds[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(reviews, training_size=0.7):\n",
    "    #reviews_new = reviews.sort_values(order_by)\n",
    "    reviews = reviews.sample(frac=1).reset_index(drop=True)\n",
    "    train_ratio= int(len(reviews)*training_size)\n",
    "    training_df = reviews.head(train_ratio)\n",
    "    validation_df = reviews[train_ratio:]\n",
    "\n",
    "    return training_df, validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = create_train_test(df, 0.8)\n",
    "train_user_item = train_df[['user_id', 'anime_id', 'rating']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'anime_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mat, movie_mat = FunkSVD(train_data_np, latent_features=3, learning_rate=0.005, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_matrix, movie_matrix, user_id, movie_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_matrix - user by latent factor matrix\n",
    "    movie_matrix - latent factor by movie matrix\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    \n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    # Create series of users and movies in the right order\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    pred =  max(min(pred,10),-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = predict_rating(user_mat, movie_mat, 1, 1)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_summary(user_id, movie_id, prediction):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    prediction - the predicted rating for user_id-movie_id\n",
    "    '''\n",
    "    prediction = max(min(predict_rating(user_mat, movie_mat, user_id, movie_id),10),-1)\n",
    "    movie_name = str(df_content[df_content['anime_id'] == movie_id]['name'].values[0])\n",
    "    movie_name = movie_name.replace('\\nName: anime, dtype: object', '').strip()\n",
    "    print(\"For User {}, we predict a {}/10 rating for the anime '{}'.\".format(user_id, round(prediction, 2), str(movie_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction_summary( 2, 12189, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.user_id == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_comparison(val_df, user_mat=user_mat, movie_mat=movie_mat):\n",
    "    '''\n",
    "    INPUT:\n",
    "    val_df - the validation dataset created in the third cell above\n",
    "    user_mat - U matrix in FunkSVD\n",
    "    movie_mat - V matrix in FunkSVD\n",
    "        \n",
    "    OUTPUT:\n",
    "    rmse - RMSE of how far off each value is from it's predicted value\n",
    "    perc_rated - percent of predictions out of all possible that could be rated\n",
    "    actual_v_pred - a 10 x 10 grid with counts for actual vs predicted values\n",
    "    '''\n",
    "        \n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['anime_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    sse = 0\n",
    "    num_rated = 0\n",
    "    preds, acts = [], []\n",
    "    actual_v_pred = np.zeros((10,10))\n",
    "    for idx in range ( len(val_users)):\n",
    "        try:\n",
    "            pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "            sse += (pred - val_ratings[idx])**2\n",
    "            num_rated+=1\n",
    "            preds.append(pred)\n",
    "            acts.append(val_ratings[idx])\n",
    "            actual_v_pred[11 - int(val_ratings[idx]-1), int(round(pred) - 1)]+=1\n",
    "        except:\n",
    "            continue\n",
    "    rmse = np.sqrt(sse/num_rated)\n",
    "    perc_rated = num_rated/len(val_users) * 100\n",
    "    return rmse, perc_rated, actual_v_pred, preds, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well did we do?\n",
    "rmse, perc_rated, actual_v_pred, preds, acts = validation_comparison(val_df)\n",
    "print(rmse, perc_rated)\n",
    "sns.heatmap(actual_v_pred);\n",
    "plt.xticks(np.arange(10), np.arange(1,11));\n",
    "plt.yticks(np.arange(10), np.arange(1,11));\n",
    "plt.xlabel(\"Predicted Values\");\n",
    "plt.ylabel(\"Actual Values\");\n",
    "plt.title(\"Actual vs. Predicted Values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(acts, density=True, alpha=.5, label='actual');\n",
    "plt.hist(preds, density=True, alpha=.5, label='predicted');\n",
    "plt.legend(loc=2, prop={'size': 15});\n",
    "plt.xlabel('Rating');\n",
    "plt.title('Predicted vs. Actual Rating');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_rated = int(len(val_df['rating'])*(1-perc_rated/100))\n",
    "rated = int(len(val_df['rating'])*perc_rated/100)\n",
    "\n",
    "print(\"Number not rated {}\".format(not_rated))\n",
    "print(\"Number rated {}.\".format(rated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW MOVIE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content_to_select = df_content[[\"anime_id\",\"genre\",\"type\"]]\n",
    "df_content_to_select[\"genre\"] = df_content_to_select[\"genre\"].fillna(\"Unknown\")\n",
    "df_content_to_select[\"genre1\"] = df_content_to_select[\"genre\"].str.split(',')\n",
    "df_content_to_select[\"genre\"] = df_content_to_select[\"genre1\"].copy()\n",
    "df_content_to_select = df_content_to_select.drop(\"genre1\", axis=1)\n",
    "df_model = pd.DataFrame(df_content_to_select[\"genre\"].tolist())\n",
    "df_content_to_select = pd.concat([df_content_to_select, df_model], axis=1)\n",
    "df_content_to_select = df_content_to_select.drop(\"genre\", axis=1).melt(id_vars=[\"anime_id\",\"type\"]).dropna(subset=[\"value\"])\n",
    "df_content_to_select[\"variable\"] = 1\n",
    "df_content_to_select = df_content_to_select.drop_duplicates()\n",
    "df_content_to_select = df_content_to_select.pivot(index = [\"anime_id\",\"type\"], columns=[\"value\"]).fillna(0)\n",
    "df_content_to_select.columns = [' '.join(col).strip().replace(\"variable \",\"\").replace(\"-\",\"_\") for col in df_content_to_select.columns.values]\n",
    "df_content_to_select.reset_index(inplace=True)\n",
    "df_content_to_select = pd.get_dummies(df_content_to_select,[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_main = df_new.reset_index()\n",
    "# df_sub = df_new[[\"name\",\"dob\",\"traits\"]].reset_index()\n",
    "# df2_sub = pd.DataFrame(df_sub[\"traits\"].str.split(\";\"))\n",
    "# df2_sub = pd.DataFrame(df2_sub[\"traits\"].values.tolist())\n",
    "# df3_sub = pd.concat([df_sub[[\"index\",\"name\",\"dob\"]],df2_sub], axis=1)\n",
    "# df3_sub = df3_sub.melt(id_vars=[\"name\",\"dob\",\"index\"])\n",
    "# df3_sub[\"variable\"] = 1\n",
    "# df3_sub = df3_sub.drop_duplicates()\n",
    "# df3_sub = df3_sub.dropna()\n",
    "# df3_sub= df3_sub.pivot(index=[\"name\",\"dob\",\"index\"], columns=[\"value\"]).fillna(0).reset_index()\n",
    "# df3_sub.columns = [' '.join(col).strip().replace(\"variable \",\"trait_\").replace(\"-\",\"_\") for col in df3_sub.columns.values]\n",
    "# df3_sub = df3_sub.drop([\"name\",\"dob\"], axis=1)\n",
    "# df_merged = df_main.merge(df3_sub, on=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content_to_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "#movie_content = np.array(movies.iloc[:,4:])\n",
    "movie_content = np.array(df_content_to_select.drop(\"anime_id\", axis=1))\n",
    "\n",
    "# Take the dot product to obtain a movie x movie matrix of similarities\n",
    "dot_prod_movies = movie_content.dot(np.transpose(movie_content))\n",
    "\n",
    "\n",
    "def find_similar_movies(movie_id, n=10):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_id - a movie_id \n",
    "    OUTPUT\n",
    "    similar_movies - an array of the most similar movies by title\n",
    "    '''\n",
    "    # find the row of each movie id\n",
    "    similar_idxs = []\n",
    "    for number in range(n) : \n",
    "        movie_idx = np.where(df_content['anime_id'] == movie_id)[0][0]\n",
    "      #  print(\"movie_id\",movie_id)\n",
    "      #  print(\"movie_idx\",movie_idx)\n",
    "    # find the most similar movie indices - to start I said they need to be the same for all content\n",
    "        movie_id = np.where(dot_prod_movies[movie_idx] == np.max(dot_prod_movies[movie_idx]))[0][0]\n",
    "\n",
    "        similar_idxs.append(movie_idx)\n",
    "    # pull the movie titles based on the indices\n",
    "    similar_movies = np.array(df_content.iloc[similar_idxs, ]['name'])\n",
    "    \n",
    "    return similar_movies\n",
    "    \n",
    "    \n",
    "def get_movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(_id, _id_type='movie', train_data=train_data_df, \n",
    "                         train_df=train_df, movies=df_content, rec_num=10, user_mat=user_mat):\n",
    "    '''\n",
    "    INPUT:\n",
    "    _id - either a user or movie id (int)\n",
    "    _id_type - \"movie\" or \"user\" (str)\n",
    "    train_data - dataframe of data as user-movie matrix\n",
    "    train_df - dataframe of training data reviews\n",
    "    movies - movies df\n",
    "    rec_num - number of recommendations to return (int)\n",
    "    user_mat - the U matrix of matrix factorization\n",
    "    movie_mat - the V matrix of matrix factorization\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (array) a list or numpy array of recommended movies like the \n",
    "                   given movie, or recs for a user_id given\n",
    "    '''\n",
    "    # if the user is available from the matrix factorization data, \n",
    "    # I will use this and rank movies based on the predicted values\n",
    "    # For use with user indexing\n",
    "    val_users = train_data_df.index\n",
    "    rec_names , rec_ids= get_top_anime(10)\n",
    "    \n",
    "    if _id_type == 'user':\n",
    "        if _id in train_data.index:\n",
    "            # Get the index of which row the user is in for use in U matrix\n",
    "            idx = np.where(val_users == _id)[0][0]\n",
    "            \n",
    "            # take the dot product of that row and the V matrix\n",
    "            preds = np.dot(user_mat[idx,:],movie_mat)\n",
    "            \n",
    "            # pull the top movies according to the prediction\n",
    "            indices = preds.argsort()[-rec_num:][::-1] #indices\n",
    "            rec_ids = train_data_df.columns[indices]\n",
    "            rec_names = get_anime_names(rec_ids)\n",
    "        else:\n",
    "            pass\n",
    "            # if we don't have this user, give just top ratings back\n",
    "    #        rec_names = popular_recommendations(_id, rec_num, rec_ids)\n",
    "    #        Settle Popular Recommendation\n",
    "    # Find similar movies if it is a movie that is passed\n",
    "    else:\n",
    "        rec_ids = find_similar_movies(_id)\n",
    "        rec_names = find_similar_movies(_id)\n",
    "    \n",
    "    return rec_ids, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs_dict_with_top = dict()\n",
    "for user_id in set(val_df['user_id']):\n",
    "    user_recs_dict_with_top[user_id] = make_recommendations(user_id, 'user')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnter = 0\n",
    "for user, rec in user_recs_dict_with_top.items():\n",
    "    if cnter < 12:\n",
    "        print(\"For user {}, our recommendations are: \\n {}\".format(user, rec))\n",
    "        cnter+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in user and movie matrices\n",
    "user_file = open(\"user_matrix\", 'rb')\n",
    "user_mat = pickle.load(user_file)\n",
    "user_file.close()\n",
    "\n",
    "\n",
    "movie_file = open(\"movie_matrix\", 'rb')\n",
    "movie_mat = pickle.load(movie_file)\n",
    "movie_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recommender as r\n",
    "\n",
    "#instantiate recommender\n",
    "rec = r.Recommender()\n",
    "\n",
    "# fit recommender\n",
    "#rec.fit(rating_pth='data/rating.csv', content_pth= 'data/anime.csv', learning_rate=.01, iters=10)\n",
    "\n",
    "# predict\n",
    "#rec.predict_rating(user_id=8, movie_id=2844)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.fit(rating_pth='data/rating.csv', content_pth= 'data/anime.csv', learning_rate=.002, iters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.predict_rating(user_id=1, movie_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.make_recommendations(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
